{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Generative Models\n",
    "\n",
    "## A: [*Bayesian Linear Regression* ](#partA); B: [*Variational Autoencoder*](#partB)\n",
    "\n",
    "[**Haiping Lu**](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  [COM4509/6509 MLAI2021](https://github.com/maalvarezl/MLAI) @ The University of Sheffield\n",
    "\n",
    "**Accompanying lectures**: [YouTube video lectures recorded in Year 2020/21.](https://www.youtube.com/watch?v=c7qt56HH_Wg&list=PLuRoUKdWifzzFL5Am_xk-NDwewFwm1-BM)\n",
    "\n",
    "**Sources**: Part A is based on the [Bayesian regression with linear basis function models](https://github.com/krasserm/bayesian-machine-learning/blob/dev/bayesian-linear-regression/bayesian_linear_regression.ipynb) notebook by [Martin Krasser](https://github.com/krasserm). Part B is based on Lab 5 of my [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) notebooks, with sources from Jaan Altosaar's [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/), Nitarshan Rajkumar's [Variational Autoencoders](https://github.com/nitarshan/variational-autoencoder/blob/master/Variational%20Autoencoder%20Tutorial.ipynb) notebook, Paul Guerrero's [Variational Autoencoders](https://github.com/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb) notebook, and PyTorch's [Variational Autoencoder code](https://github.com/pytorch/examples/blob/master/vae/main.py).\n",
    "\n",
    "There are *three* questions in this notebook.\n",
    "\n",
    "**Suggested reading**: \n",
    "* [Bayesian linear regression on Wiki](https://en.wikipedia.org/wiki/Bayesian_linear_regression)\n",
    "* Bayesian linear regression (Section 3.3) in the [PRML book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "* [Variational Bayesian methods on Wiki](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)\n",
    "* [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "* [Variational autoencoders notes](https://deepgenerativemodels.github.io/notes/vae/)\n",
    "* [**Troubleshooting Deep Neural Networks - Josh Tobin**](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partA'></a>A: Bayesian Linear Regression\n",
    "\n",
    "## Why\n",
    "\n",
    "[**Generative models**](https://en.wikipedia.org/wiki/Generative_model) can estimate the probability of the instance, and also the probability of a class label. One of their important applications is [uncertainty quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification). \n",
    "\n",
    "We will start from the generative version of linear regression, our most basic machine learning model. The classic generative formulation of linear regression is called **Bayesian linear regression** or simply **Bayesian regression**.  \n",
    "\n",
    "## A1. Recap of linear basis function models\n",
    "\n",
    "*Note that the notations could be different from previous notations.*\n",
    "\n",
    "Linear regression models share the property of being linear in their parameters but not necessarily in their input variables. Using non-linear basis functions of input variables, linear models are able model arbitrary non-linearities from input variables to targets. Polynomial regression is such an example and will be demonstrated later. A linear regression model $y(\\mathbf{x}, \\mathbf{w})$ can therefore be defined more generally as\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1}{w_j \\phi_j(\\mathbf{x})} = \\sum_{j=0}^{M-1}{w_j \\phi_j(\\mathbf{x})} = \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}) \\tag{1}\n",
    "$$\n",
    "\n",
    "where $\\phi_j$ are basis functions and $M$ is the total number of parameters $w_j$ including the bias term $w_0$. Here, we use the convention $\\phi_0(\\mathbf{x}) = 1$. The simplest form of linear regression models are also linear functions of their input variables i.e. the set of basis functions in this case is the identity $\\boldsymbol\\phi(\\mathbf{x}) = \\mathbf{x}$. The target variable $t$ of an observation $\\mathbf{x}$ is given by a deterministic function $y(\\mathbf{x}, \\mathbf{w})$ plus additive random noise $\\epsilon$. \n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x}, \\mathbf{w}) + \\epsilon \\tag{2}\n",
    "$$\n",
    "\n",
    "We make the assumption that the noise is normally distributed i.e. follows a Gaussian distribution with zero mean and precision (= inverse variance) $\\beta$. The corresponding probabilistic model i.e. the conditional distribution of $t$ given $\\mathbf{x}$ can therefore be written as\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta) = \n",
    "\\mathcal{N}(t \\lvert y(\\mathbf{x}, \\mathbf{w}), \\beta^{-1}) =\n",
    "\\sqrt{\\beta \\over {2 \\pi}} \\exp\\left(-{\\beta \\over 2} (t - y(\\mathbf{x}, \\mathbf{w}))^2 \\right) \\tag{3}\n",
    "$$\n",
    "\n",
    "where the mean of this distribution is the regression function $y(\\mathbf{x}, \\mathbf{w})$. \n",
    "\n",
    "### Likelihood function\n",
    "\n",
    "For fitting the model and for inference of model parameters we use a training set of $N$ independent and identically distributed (i.i.d.) observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N$ and their corresponding targets $t_1,\\ldots,t_N$. After combining column vectors $\\mathbf{x}_i$ into matrix $\\mathbf{X}$, where $\\mathbf{X}_{i,:} = \\mathbf{x}_i^T$, and scalar targets $t_i$ into column vector $\\mathbf{t}$ the joint conditional probability of targets $\\mathbf{t}$ given $\\mathbf{X}$ can be formulated as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{t} \\lvert \\mathbf{X}, \\mathbf{w}, \\beta) = \n",
    "\\prod_{i=1}^{N}{\\mathcal{N}(t_i \\lvert \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}_i), \\beta^{-1})} \\tag{4}\n",
    "$$\n",
    "\n",
    "This is a function of parameters $\\mathbf{w}$ and $\\beta$ and is called the *likelihood function*. For better readability, it will be written as $p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta)$ instead of $p(\\mathbf{t} \\lvert \\mathbf{X}, \\mathbf{w}, \\beta)$ from now on. The log of the likelihood function can be written as \n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta) = \n",
    "{N \\over 2} \\log \\beta - \n",
    "{N \\over 2} \\log {2 \\pi} - \n",
    "\\beta E_D(\\mathbf{w}) \\tag{5}\n",
    "$$\n",
    "\n",
    "where $E_D(\\mathbf{w})$ is the sum-of-squares error function coming from the exponent of the likelihood function.\n",
    "\n",
    "$$\n",
    "E_D(\\mathbf{w}) = \n",
    "{1 \\over 2} \\sum_{i=1}^{N}(t_i - \\mathbf{w}^T \\boldsymbol\\phi(\\mathbf{x}_i))^2 = \n",
    "{1 \\over 2} \\lVert \\mathbf{t} - \\boldsymbol\\Phi \\mathbf{w} \\rVert^2 \\tag{6}\n",
    "$$\n",
    "\n",
    "Matrix $\\boldsymbol\\Phi$ is called the *design matrix* and is defined as\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\Phi = \n",
    "\\begin{pmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &  \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\ \n",
    "\\phi_0(\\mathbf{x}_2) &  \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_0(\\mathbf{x}_N) &  \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N)\n",
    "\\end{pmatrix} \\tag{7}\n",
    "$$\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "Maximizing the log likelihood (= minimizing the sum-of-squares error function) w.r.t. $\\mathbf{w}$ gives the maximum likelihood estimate of parameters $\\mathbf{w}$. Maximum likelihood estimation can lead to severe over-fitting if complex models (e.g. polynomial regression models of high order) are fit to datasets of limited size. A common approach to prevent over-fitting is to add a regularization term to the error function. As we will see shortly, this regularization term arises naturally when following a Bayesian approach (more precisely, when defining a prior distribution over parameters $\\mathbf{w}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Bayesian approach to linear regression\n",
    "\n",
    "### Prior and posterior distribution\n",
    "\n",
    "For a Bayesian treatment of linear regression we need a prior probability distribution over model parameters $\\mathbf{w}$. For reasons of simplicity, we will use an isotropic Gaussian distribution over parameters $\\mathbf{w}$ with zero mean:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} \\lvert \\alpha) = \\mathcal{N}(\\mathbf{w} \\lvert \\mathbf{0}, \\alpha^{-1}\\mathbf{I}) \\tag{8}\n",
    "$$\n",
    "\n",
    "An isotropic Gaussian distribution has a diagonal covariance matrix where all diagonal elements have the same variance $\\alpha^{-1}$ ($\\alpha$ is the precision of the prior). A zero mean favors small(er) values of parameters $w_j$ a priori. The prior is [conjugate](https://en.wikipedia.org/wiki/Conjugate_prior) to the likelihood $p(\\mathbf{t} \\lvert \\mathbf{w}, \\beta)$ meaning that the posterior distribution has the same functional form as the prior i.e. is also a Gaussian. In this special case, the posterior has an analytical solution with the following [sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic). \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_N &= \\beta \\mathbf{S}_N \\boldsymbol\\Phi^T \\mathbf{t}  \\tag{9} \\\\\n",
    "\\mathbf{S}_N^{-1} &= \\alpha\\mathbf{I} + \\beta \\boldsymbol\\Phi^T \\boldsymbol\\Phi  \\tag{10}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$(9)$ is the mean vector of the posterior and $(10)$ the inverse covariance matrix (= precision matrix). Hence, the posterior distribution can be written as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) = \\mathcal{N}(\\mathbf{w} \\lvert \\mathbf{m}_N, \\mathbf{S}_N) \\tag{11}\n",
    "$$\n",
    "\n",
    "For the moment, we assume that the values of $\\alpha$ and $\\beta$ are known. Since the posterior is proportional to the product of likelihood and prior, the log of the posterior distribution is proportional to the sum of the log likelihood and the log of the prior\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) = \n",
    "-\\beta E_D(\\mathbf{w}) - \\alpha E_W(\\mathbf{w}) + \\mathrm{const.} \\tag{12}\n",
    "$$\n",
    "\n",
    "where $E_D(\\mathbf{w})$ is defined by $(6)$ and \n",
    "\n",
    "$$\n",
    "E_W(\\mathbf{w}) = {1 \\over 2} \\mathbf{w}^T \\mathbf{w} \\tag{13}\n",
    "$$\n",
    "\n",
    "Maximizing the log posterior w.r.t. $\\mathbf{w}$ gives the [maximum-a-posteriori](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) estimate of $\\mathbf{w}$. Maximizing the log posterior is equivalent to minimizing the sum-of-squares error function $E_D$ plus a quadratic regularization term $E_W$. This particular form regularization is known as *L2 regularization* or *weight decay* as it limits the magnitude of weights $w_j$. The contribution of the regularization term is determined by the ratio $\\alpha / \\beta$.\n",
    "\n",
    "### Posterior predictive distribution\n",
    "\n",
    "For making a prediction $t$ at a new location $\\mathbf{x}$ we use the posterior predictive distribution which is defined as\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) = \n",
    "\\int{p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta) p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta) d\\mathbf{w}} \\tag{14}\n",
    "$$\n",
    "\n",
    "The posterior predictive distribution includes uncertainty about parameters $\\mathbf{w}$ into predictions by weighting the conditional distribution $p(t \\lvert \\mathbf{x}, \\mathbf{w}, \\beta)$ with the posterior probability of weights $p(\\mathbf{w} \\lvert \\mathbf{t}, \\alpha, \\beta)$ over the entire weight parameter space. By using the predictive distribution we're not only getting the expected value of $t$ at a new location $\\mathbf{x}$ but also the **uncertainty for that prediction**. In our special case, the posterior predictive distribution is a Gaussian distribution\n",
    "\n",
    "$$\n",
    "p(t \\lvert \\mathbf{x}, \\mathbf{t}, \\alpha, \\beta) = \n",
    "\\mathcal{N}(t \\lvert \\mathbf{m}_N^T \\boldsymbol\\phi(\\mathbf{x}), \\sigma_N^2(\\mathbf{x})) \\tag{15}\n",
    "$$\n",
    "\n",
    "where mean $\\mathbf{m}_N^T \\boldsymbol\\phi(\\mathbf{x})$ is the regression function after $N$ observations and $\\sigma_N^2(\\mathbf{x})$ is the corresponding predictive variance\n",
    "\n",
    "$$\n",
    "\\sigma_N^2(\\mathbf{x}) = {1 \\over \\beta} + \\boldsymbol\\phi(\\mathbf{x})^T \\mathbf{S}_N \\boldsymbol\\phi(\\mathbf{x}) \\tag{16}\n",
    "$$\n",
    "\n",
    "The first term in $(16)$ represents the inherent noise in the data and the second term covers the uncertainty about parameters $\\mathbf{w}$. So far, we have assumed that the values of $\\alpha$ and $\\beta$ are known. In a fully Bayesian treatment, however, we should define priors over $\\alpha$ and $\\beta$ and use the corresponding posteriors to additionally include uncertainties about $\\alpha$ and $\\beta$ into predictions, which will not be covered here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Implementation of Bayesian regression\n",
    "\n",
    "### Posterior and posterior predictive distribution\n",
    "\n",
    "We start with the implementation of the posterior and posterior predictive distributions. Function `posterior` computes the mean and covariance matrix of the posterior distribution and function `posterior_predictive` computes the mean and the variances of the posterior predictive distribution. Here, readability of code and similarity to the mathematical definitions has higher priority than optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def posterior(Phi, t, alpha, beta, return_inverse=False):\n",
    "    \"\"\"Computes mean and covariance matrix of the posterior distribution.\"\"\"\n",
    "    S_N_inv = alpha * np.eye(Phi.shape[1]) + beta * Phi.T.dot(Phi)\n",
    "    S_N = np.linalg.inv(S_N_inv)\n",
    "    m_N = beta * S_N.dot(Phi.T).dot(t)\n",
    "\n",
    "    if return_inverse:\n",
    "        return m_N, S_N, S_N_inv\n",
    "    else:\n",
    "        return m_N, S_N\n",
    "\n",
    "def posterior_predictive(Phi_test, m_N, S_N, beta):\n",
    "    \"\"\"Computes mean and variances of the posterior predictive distribution.\"\"\"\n",
    "    y = Phi_test.dot(m_N)\n",
    "    # Only compute variances (diagonal elements of covariance matrix)\n",
    "    y_var = 1 / beta + np.sum(Phi_test.dot(S_N) * Phi_test, axis=1)\n",
    "    \n",
    "    return y, y_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some plotting functions for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, t):\n",
    "    plt.scatter(x, t, marker='o', c=\"k\", s=20)\n",
    "\n",
    "def plot_truth(x, y, label='Truth'):\n",
    "    plt.plot(x, y, 'k--', label=label)\n",
    "\n",
    "def plot_predictive(x, y, std, y_label='Prediction', std_label='Uncertainty', plot_xy_labels=True):\n",
    "    y = y.ravel()\n",
    "    std = std.ravel()\n",
    "\n",
    "    plt.plot(x, y, label=y_label)\n",
    "    plt.fill_between(x.ravel(), y + std, y - std, alpha = 0.5, label=std_label)\n",
    "\n",
    "    if plot_xy_labels:\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "\n",
    "def plot_posterior_samples(x, ys, plot_xy_labels=True):\n",
    "    plt.plot(x, ys[:, 0], 'r-', alpha=0.5, label='Post. samples')\n",
    "    for i in range(1, ys.shape[1]):\n",
    "        plt.plot(x, ys[:, i], 'r-', alpha=0.5)\n",
    "\n",
    "    if plot_xy_labels:\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "\n",
    "def plot_posterior(mean, cov, w0, w1):\n",
    "    resolution = 100\n",
    "\n",
    "    grid_x = grid_y = np.linspace(-1, 1, resolution)\n",
    "    grid_flat = np.dstack(np.meshgrid(grid_x, grid_y)).reshape(-1, 2)\n",
    "\n",
    "    densities = stats.multivariate_normal.pdf(grid_flat, mean=mean.ravel(), cov=cov).reshape(resolution, resolution)\n",
    "    plt.imshow(densities, origin='lower', extent=(-1, 1, -1, 1))\n",
    "    plt.scatter(w0, w1, marker='x', c=\"r\", s=20, label='Truth')\n",
    "\n",
    "    plt.xlabel('w0')\n",
    "    plt.ylabel('w1')\n",
    "\n",
    "def print_comparison(title, a, b, a_prefix='np', b_prefix='br'):\n",
    "    print(title)\n",
    "    print('-' * len(title))\n",
    "    print(f'{a_prefix}:', a)\n",
    "    print(f'{b_prefix}:', b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example datasets\n",
    "\n",
    "The datasets used in the following examples are based on $N$ scalar observations $x_{i = 1,\\ldots,N}$ which are combined into a $N \\times 1$ matrix $\\mathbf{X}$. Target values $\\mathbf{t}$ are generated from $\\mathbf{X}$ with functions `f` and `g` which also generate random noise whose variance can be specified with the `noise_variance` parameter. We will use `f` for generating noisy samples from a straight line and `g` for generating noisy samples from a sinusoidal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w0 = -0.3\n",
    "f_w1 =  0.5\n",
    "\n",
    "def f(X, noise_variance):\n",
    "    '''Linear function plus noise'''\n",
    "    return f_w0 + f_w1 * X + noise(X.shape, noise_variance)\n",
    "\n",
    "def g(X, noise_variance):\n",
    "    '''Sinusoidial function plus noise'''\n",
    "    return 0.5 + np.sin(2 * np.pi * X) + noise(X.shape, noise_variance)\n",
    "\n",
    "def noise(size, variance):\n",
    "    return np.random.normal(scale=np.sqrt(variance), size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis functions\n",
    "\n",
    "For straight line fitting, a model that is linear in its input variable $x$ is sufficient. Hence, we don't need to transform $x$ with a basis function which is equivalent to using an `identity_basis_function`. For fitting a linear model to a sinusoidal dataset we transform input $x$ with `gaussian_basis_function` and later with `polynomial_basis_function`. These non-linear basis functions are necessary to model the non-linear relationship between input $x$ and target $t$. The design matrix $\\boldsymbol\\Phi$ can be computed from observations $\\mathbf{X}$ and a parametric basis function with function `expand`. This function also prepends a column vector $\\mathbf{1}$ according to $\\phi_0(x) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_basis_function(x):\n",
    "    return x\n",
    "\n",
    "def gaussian_basis_function(x, mu, sigma=0.1):\n",
    "    return np.exp(-0.5 * (x - mu) ** 2 / sigma ** 2)\n",
    "\n",
    "def polynomial_basis_function(x, power):\n",
    "    return x ** power\n",
    "\n",
    "def expand(x, bf, bf_args=None):\n",
    "    if bf_args is None:\n",
    "        return np.concatenate([np.ones(x.shape), bf(x)], axis=1)\n",
    "    else:\n",
    "        return np.concatenate([np.ones(x.shape)] + [bf(x, bf_arg) for bf_arg in bf_args], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight line fitting\n",
    "\n",
    "For straight line fitting, we use a linear regression model of the form $y(x, \\mathbf{w}) = w_0 + w_1 x$ and do Bayesian inference for model parameters $\\mathbf{w}$. Predictions are made with the posterior predictive distribution. Since this model has only two parameters, $w_0$ and $w_1$, we can visualize the posterior density in 2D which is done in the first column of the following output. Rows use an increasing number of training data from a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset sizes\n",
    "N_list = [1, 3, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1) * 2 - 1\n",
    "\n",
    "# Training target values\n",
    "t = f(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = f(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, identity_basis_function)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, identity_basis_function)\n",
    "    \n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 3, i * 3 + 1)\n",
    "    plot_posterior(m_N, S_N, f_w0, f_w1)\n",
    "    plt.title(f'Posterior density (N = {N})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(len(N_list), 3, i * 3 + 3)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.5, 1.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second column, 5 random weight samples are drawn from the posterior and the corresponding regression lines are plotted in red color. The line resulting from the true parameters, `f_w0` and `f_w1` is plotted as dashed black line and the noisy training data as black dots. The third column shows the mean and the standard deviation of the posterior predictive distribution along with the true model and the training data.\n",
    "\n",
    "It can be clearly seen how the posterior density in the first column gets more sharply peaked as the size of the dataset increases which corresponds to a decrease in the sample variance in the second column and to a decrease in prediction uncertainty as shown in the third column. Also note how prediction uncertainty is higher in regions of less observations. \n",
    "\n",
    "### Gaussian basis functions\n",
    "\n",
    "The following example demonstrates how to fit a Gaussian basis function model to a noisy sinusoidal dataset. It uses 9 Gaussian basis functions with mean values equally distributed over $[0, 1]$ each having a standard deviation of $0.1$. Inference for parameters $\\mathbf{w}$ is done in the same way as in the previous example except that we now infer values for 10 parameters (bias term $w_0$ and $w_1,\\ldots,w_9$ for the 9 basis functions) instead of 2. We therefore cannot display the posterior density unless we selected 2 parameters at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [3, 8, 20]\n",
    "\n",
    "beta = 25.0\n",
    "alpha = 2.0\n",
    "\n",
    "# Training observations in [-1, 1)\n",
    "X = np.random.rand(N_list[-1], 1)\n",
    "\n",
    "# Training target values\n",
    "t = g(X, noise_variance=1/beta)\n",
    "\n",
    "# Test observations\n",
    "X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Function values without noise \n",
    "y_true = g(X_test, noise_variance=0)\n",
    "    \n",
    "# Design matrix of test observations\n",
    "Phi_test = expand(X_test, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    X_N = X[:N]\n",
    "    t_N = t[:N]\n",
    "\n",
    "    # Design matrix of training observations\n",
    "    Phi_N = expand(X_N, bf=gaussian_basis_function, bf_args=np.linspace(0, 1, 9))\n",
    "\n",
    "    # Mean and covariance matrix of posterior\n",
    "    m_N, S_N = posterior(Phi_N, t_N, alpha, beta)\n",
    "    \n",
    "    # Mean and variances of posterior predictive \n",
    "    y, y_var = posterior_predictive(Phi_test, m_N, S_N, beta)\n",
    "    \n",
    "    # Draw 5 random weight samples from posterior and compute y values\n",
    "    w_samples = np.random.multivariate_normal(m_N.ravel(), S_N, 5).T\n",
    "    y_samples = Phi_test.dot(w_samples)\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 1)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true)\n",
    "    plot_posterior_samples(X_test, y_samples)\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(len(N_list), 2, i * 2 + 2)\n",
    "    plot_data(X_N, t_N)\n",
    "    plot_truth(X_test, y_true, label=None)\n",
    "    plot_predictive(X_test, y, np.sqrt(y_var))\n",
    "    plt.ylim(-1.0, 2.0)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as the size of the dataset increases the posterior sample variance and the prediction uncertainty decreases. Also, regions with less observations have higher prediction uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partB'></a>B: Variational Autoencoder for Deep Generative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "**Deep generative models** aim to combine the interpretable representations and quantified uncertainty offered by probabilistic models, with the flexibility and scalable learning of deep neural networks, from the [Deep Generative Models](http://stat.columbia.edu/~cunningham/teaching/GR8201/) course by [John P. Cunningham](http://stat.columbia.edu/~cunningham/). \n",
    "\n",
    "**Variational autoencoders** are such deep generative models. They let us design complex generative models of data, and fit them to large datasets. They yield state-of-the-art machine learning results in image generation and reinforcement learning and can generate [images of fictional celebrity faces](https://www.youtube.com/watch?v=XNZIN7Jh3Sg) and [high-resolution digital artwork](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Neural network architecture\n",
    "\n",
    "Like all autoencoders, the variational autoencoder is primarily used for unsupervised learning of hidden/latent representations. However, they are fundamentally different in that they approach the problem from a probabilistic perspective. They specify a joint distribution over the observed and latent variables, and approximate the intractable posterior conditional density over latent variables with [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods).\n",
    "\n",
    "A variational autoencoder consists of an encoder, a decoder, and a loss function as a typical autoencoder: \n",
    "\n",
    "<img src=\"https://jaan.io/images/encoder-decoder.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 120px\" />\n",
    "     \n",
    "* The **encoder** takes a datapoint $x$ as the input and encodes it to produce a hidden/latent representation $z$ as the output, with parameters $\\theta$ (weights and biases). The encoder in a VAE can be written as a Gaussian probability density $q_\\theta (z \\mid x)$ to model a stochastic lower-dimensional space is stochastic. The encoder outputs parameters to $q_\\theta (z \\mid x)$ and we can sample from this distribution to get noisy values of the representations $z$. \n",
    "* The **decoder** $p_\\phi(x\\mid z)$ takes the representation $z$ as the input and outputs the parameters to the probability distribution of the data $p_\\phi (x\\mid z)$, with parameters $\\phi$ (weights and biases). The reconstruction log-likelihood $\\log p_\\phi (x\\mid z)$ tells us how effectively the decoder has learned to reconstruct (autoencode) an input image $x$ given its latent representation $z$.\n",
    "\n",
    "* <p>The **loss function** is the negative log-likelihood with a regularizer. The loss function $l_i$ for a single datapoint $x_i$ is:\n",
    "\\begin{equation}\n",
    "\\:\\\\\n",
    "l_i(\\theta, \\phi) = - \\mathbb{E}_{z\\sim q_\\theta(z\\mid x_i)}[\\log p_\\phi(x_i\\mid z)] + \\mathbb{KL}(q_\\theta(z\\mid x_i) \\mid\\mid p(z))\\\\\n",
    "\\tag{1}\\end{equation}\n",
    "Datapoints are assumed to be independent so the total loss is $\\sum_{i=1}^N l_i$ for $N$ total datapoints. The first term aims for reconstruction and is defined as the expected negative log-likelihood of the $i$-th datapoint, with the expectation taken w.r.t. to the encoder’s distribution over the representations. The second term is a regularizer in the form of the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the encoder’s\n",
    "distribution $q_\\theta(z\\mid x)$ (the approximation) and $p(z)$ (the ground truth), measuring their mismatch.\n",
    " \n",
    "The variational autoencoder specifies $p$ as a standard Gaussian distribution with mean zero and variance one. If the encoder outputs representations $z$ having a distribution different from a standard Gaussian, there will be a penalty (proportional to the amount of difference) in the loss. The training of VAE uses gradient descent to optimize the loss with respect to the parameters of the encoder and decoder $\\theta$ and $\\phi$. \n",
    "\n",
    "**Example**: A $28\\times 28$ handwritten digit image can be represented as $0$ or $1$ (black or white). We use a Bernoulli distribution to represent the probability distribution of a single pixel. The decoder receives the latent representation of a digit $z$ and outputs $784$ Bernoulli parameters, one for each of the $784$ pixels, from which we can **sample** or **generate** digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Probabilistic modelling in variational autoencoder \n",
    "\n",
    "*This section is **optional**. You may skip it safely if you are only interested in using VAE.*\n",
    "\n",
    "From a probability model perspective, a variational autoencoder is a latent variable model that **generates** a datapoint $x$ from latent variables $z$ with a joint probability $p(x, z) = p(x \\mid z) p(z)$. \n",
    "<img src=\"https://jaan.io/images/graphical-model-variational-autoencoder.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 100px\" />\n",
    "For each datapoint $i$, latent variable $z_i$ are first drawn from a prior distribution $p(z)$: $z_i \\sim p(z)$. Then, a datapoint $x_i$ is drawn from the likelihood $p(x\\mid z)$: $x_i \\sim p(x\\mid z)$. Recall that we have $p(x,z) = p(x\\mid z)p(z)$. For a black and white digit image, the likelihood has a Bernoulli distribution.\n",
    "\n",
    "With a Bayesian approach, the **inference** task is to learn good values of the latent variables given observed data in terms of the posterior\n",
    "\n",
    "$$p(z \\mid x) = \\frac{p(x \\mid z)p(z)}{p(x)}.$$\n",
    "\n",
    "The denominator $p(x)$ is the marginal distribution of $x$ (the evidence): $p(x) = \\int p(x \\mid z) p(z) dz$. This integral requires exponential time to compute so we need to an approximation.  \n",
    "\n",
    "**Variational inference** approximates the posterior with a family of distributions $q_\\lambda(z \\mid x)$ indexed by the variational parameter(s) $\\lambda$, which are the mean and variance for Gaussian distributions: $\\lambda = (\\mu, \\sigma^2))$. We can use the **Kullback-Leibler divergence** to measure how well the variational posterior $q(z \\mid x)$ approximates the true posterior $p(z \\mid x)$: \n",
    "\n",
    "$$\\mathbb{KL}(q_\\lambda(z \\mid x) \\mid \\mid p(z \\mid x)) = \\mathbf{E}_q[\\log q_\\lambda(z \\mid x)]- \\mathbf{E}_q[\\log p(x, z)] + \\log p(x)$$\n",
    "\n",
    "Optimizing the KL divergence above is intractable. However, if we define the **Evidence Lower BOund** (ELBO)\n",
    "\n",
    "$$ELBO(\\lambda) = \\mathbf{E}_q[\\log p(x, z)] - \\mathbf{E}_q[\\log q_\\lambda(z \\mid x)],$$\n",
    "\n",
    "then we can combine this with the KL divergence and rewrite the evidence (marginal distribution) as</p>\n",
    "\n",
    "$$\\log p(x) = ELBO(\\lambda) + \\mathbb{KL}(q_\\lambda(z \\mid x) \\mid \\mid p(z \\mid x))$$\n",
    "\n",
    "By Jensen’s inequality, the KL divergence is always non-negative so minimizing this KL divergence is equivalent to maximizing the ELBO because $p(x)$ is fixed (though intractable). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "Why is $ELBO(\\lambda)$ the lower bound for $\\log p(x)$?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO enables approximate posterior inference.Maximizing the ELBO is equivalent (but computationally tractable) to minimizing the KL divergence between the approximate and exact posteriors.\n",
    "\n",
    "The variational autoencoder model further assumes that no datapoint shares its latent $z$ with another datapoint so that we can decompose the ELBO as a sum of the ELBOs for each single datapoint:\n",
    "\n",
    "$$ELBO_i(\\lambda) = \\mathbb{E}_{q_\\lambda(z\\mid x_i)}[\\log p(x_i\\mid z)] - \\mathbb{\\mathbb{KL}}(q_\\lambda(z\\mid x_i) \\mid\\mid p(z)).$$\n",
    "\n",
    "Now, we can use stochastic gradient descent with respect to the parameters $\\lambda$ (which are shared across datapoints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "Verify that this definition for $ELBO_i(\\lambda)$ is equivalent to our previous definition of the ELBO. \n",
    "\n",
    "*Hint: expand the log joint into the prior and likelihood terms and use the product rule for the logarithm.*\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we parametrize the approximate posterior $q_\\theta (z \\mid x, \\lambda)$ with an **inference network** (encoder) that takes data $x$ in and outputs parameters $\\lambda$, with learnable parameter $\\theta$ (weights and biases). And we parameterize the likelihood $p(x \\mid z)$ with a **generative network** (decoder) that takes latent variables in and outputs parameters to the data distribution $p_\\phi(x \\mid z)$, with learnable parameters $\\phi$ (weights and biases). $\\theta$ and $\\phi$ can be learned by maximizing the ELBO using stochastic gradient descent with minibatch. We can write the ELBO and include the inference and generative network parameters as:</p>\n",
    "\n",
    "$$ELBO_i(\\theta, \\phi) = \\mathbb{E}_{q_\\theta(z\\mid x_i)}[\\log p_\\phi(x_i\\mid z)] - \\mathbb{KL}(q_\\theta(z\\mid x_i) \\mid\\mid p(z)).$$\n",
    "\n",
    "Notice that this evidence lower bound is the negative of the loss function $l_i(\\theta, \\phi)$ defined in Equation (1): $ELBO_i(\\theta, \\phi) = -l_i(\\theta, \\phi)$. \n",
    "\n",
    "The probability model approach reveals that two terms in Equation (1) aim to minimize the KL divergence (mismatch) between the approximate posterior $q_\\lambda(z \\mid x)$ and (true) model posterior $p(z \\mid x)$.\n",
    "\n",
    "**Summary**: In this variational inference, we've defined a probability model $p$ of latent variables and data, as well as a variational family $q$ for the latent variables to approximate our posterior. Then the variational inference algorithm can learn the variational parameters through gradient *ascent* on the ELBO. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Reparameterization trick\n",
    "\n",
    "To implement the variational autoencoder, we need to take derivatives w.r.t. the parameters of a stochastic variable. If $z$  is drawn from a distribution $q_\\theta (z \\mid x)$, how to take derivatives of a function of $z$ with respect to $\\theta$? The  sample $z$ is fixed, but intuitively its derivative should be nonzero.\n",
    "\n",
    "For Gaussian distributions, we can reparametrize samples in a clever way so that the stochasticity is independent of the parameters, e.g. by making samples to deterministically depend on the parameters of the distribution. For example, in a Gaussian variable with mean $\\mu$ and standard devation $\\sigma$, we can sample from it as:</p>\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon,$$\n",
    "\n",
    "<p>where $\\epsilon \\sim \\mathcal{N}(0, 1)$. In this way, we have defined a function which depends on the parameters deterministically so that we can take derivatives of functions involving $z$, $f(z)$ w.r.t. to the parameters of its distribution $\\mu$ and $\\sigma$, which are output by an inference network with parameters $\\theta$ that we optimize. \n",
    "\n",
    "<img src=\"https://jaan.io/images/reparametrization.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height: 120px\" />\n",
    "\n",
    "Through this trick, we can backpropagate w.r.t. $\\theta$ through the objective (the ELBO) which is a function of samples of the latent variables $z$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Variational Autoencoder via PyTorch\n",
    "\n",
    "Let us implement a VAE in PyTorch to learn a generative model for handwritten digit images from the  MNIST dataset. We use the following common setup: a multivariate Gaussian distribution for the conditional distribution $q_{\\theta}(z | x)$  and a multivariate Bernoulli distribution for the conditional distribution $p_{\\phi}(x | z)$, which reduces the reconstruction loss (the first term in Equation (1)) to the pixel-wise binary cross-entropy loss. See the [original VAE paper](https://arxiv.org/pdf/1312.6114.pdf) for details.\n",
    "\n",
    "#### Set up and get ready to build a VAE\n",
    "First, get ready by importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tnrange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid \n",
    "\n",
    "torch.manual_seed(2020)\n",
    "sns.set_style('dark')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the experiments, including data loaders. MNIST images are gray-level images with pixel values in [0,1], which will be treated as probabilities of being white (0 probability means black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128   # input batch size for training (default: 128)\n",
    "num_epochs=10    # number of epochs to train (default: 10)')\n",
    "num_visual=16    # For compare original and constructed digits\n",
    "num_grid=20      # For visualising generated digits in grid\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU if you have one\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "train_loader = DataLoader(datasets.MNIST('data', train=True, download=True, \n",
    "        transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(datasets.MNIST('data', train=False, \n",
    "        transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the VAE\n",
    "\n",
    "Now construct the VAE model and do an inspection. We use fully connected layers here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,latent_dim=20,hidden_dim=500):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim) # mean\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim) #logvar\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "myVAE = VAE().to(device)\n",
    "print(myVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and choose the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(myVAE.parameters(), lr=learning_rate)\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return (BCE + KLD) / recon_x.size(0) # normalize by batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, testing and visualisation\n",
    "\n",
    "Let us train the model and plot the losses against batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model,optimizer,dataloader):\n",
    "    model.train() # set to training mode\n",
    "    losses = []\n",
    "    for epoch in tnrange(num_epochs,desc='Epochs'):\n",
    "        for data, _ in dataloader:        \n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item()) # train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "    return losses\n",
    "\n",
    "train_losses = train(myVAE,optimizer,train_loader)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom in to the converging region and compute the moving average of losses to observe the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize moving average of losses\n",
    "def visualize_losses_moving_average(losses,window=50,boundary='valid',ylim=(95,125)):\n",
    "    mav_losses = np.convolve(losses,np.ones(window)/window,boundary)\n",
    "    corrected_mav_losses = np.append(np.full(window-1,np.nan),mav_losses)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(losses)\n",
    "    plt.plot(corrected_mav_losses)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n",
    "visualize_losses_moving_average(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test(model,dataloader):\n",
    "    model.eval() # set to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataloader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            loss = loss_function(recon_batch, data, mu, logvar).item()\n",
    "            test_loss += loss*data.size(0)\n",
    "    return test_loss/len(dataloader.dataset)\n",
    "\n",
    "test_loss = test(myVAE, test_loader)\n",
    "print(test_loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the images reconstructed from the learned latent representations with the original images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VAE input and reconstruction\n",
    "def visualize_mnist_vae(model,dataloader,num=num_visual):\n",
    "    def imshow(img):\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        data,_ = iter(dataloader).next()\n",
    "        data = data.to(device)\n",
    "        data = data[0:num,:,:]        \n",
    "        recon_batch,_,_ = model(data)\n",
    "        recon_batch = recon_batch.data.view(num, 1, 28, 28)\n",
    "        imshow(make_grid(data))\n",
    "        imshow(make_grid(recon_batch))\n",
    "\n",
    "visualize_mnist_vae(myVAE,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D latent space studies\n",
    "Let us set the latent dimension to 2 for easy visualisation. With this setting, the reconstruction is poorer than higher latent dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test and visualize reconstruction using a 2D latent space\n",
    "myVAE2 = VAE(latent_dim=2)\n",
    "optimizer2 = optim.Adam(myVAE2.parameters(), lr=learning_rate)\n",
    "\n",
    "train2_losses = train(myVAE2,optimizer2,train_loader)\n",
    "test2_loss = test(myVAE2,test_loader)\n",
    "\n",
    "print(test2_loss)\n",
    "visualize_losses_moving_average(train2_losses,ylim=(135,175))\n",
    "visualize_mnist_vae(myVAE2,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the latent representations in a 2D space with the ground truth class labels to observe the *clustering* effects: the latent representations for the different classes of digits are placed in different parts of the latent space in most cases, though with overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test data encodings on the latent space\n",
    "def visualize_encoder(model,dataloader):\n",
    "    z_means_x, z_means_y, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images,labels in iter(dataloader):\n",
    "            z_means,_ = model.encode(images.view(-1,784)) # We visualise the mean\n",
    "            z_means_x = np.append(z_means_x,z_means[:,0].data.numpy())\n",
    "            z_means_y = np.append(z_means_y,z_means[:,1].data.numpy())\n",
    "            all_labels = np.append(all_labels,labels.numpy())\n",
    "\n",
    "    plt.figure(figsize=(6.5,5))\n",
    "    plt.scatter(z_means_x,z_means_y,c=all_labels,cmap='inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "visualize_encoder(myVAE2,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE as a digit image generator\n",
    "\n",
    "Using the learned generative VAE model, we can sample/generate new digit images via sampling their latent vectors at a regular grid from its prior distribution. We display them along the 2D latent space manifold. We can observe interesting patterns on the regularly spaced grid and interpret what each dimension of the latent space has captured. Although the generated digits are not perfect and have artefacts, they are usually better than for a non-variational Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize digits generated from latent space grid\n",
    "def visualize_decoder(model,num=num_grid,range_type='g'):\n",
    "    image_grid = np.zeros([num*28,num*28])\n",
    "\n",
    "    if range_type == 'l': # linear range\n",
    "        # corresponds to output range of visualize_encoding()\n",
    "        range_space = np.linspace(-4,4,num)\n",
    "    elif range_type == 'g': # gaussian range\n",
    "        range_space = norm.ppf(np.linspace(0.01,0.99,num))\n",
    "    else:\n",
    "        range_space = range_type\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(range_space):\n",
    "            for j, y in enumerate(reversed(range_space)):\n",
    "                z = torch.FloatTensor([[x,y]])\n",
    "                image = model.decode(z).view(-1,1,28,28)\n",
    "                image = image.data.numpy()\n",
    "                image_grid[(j*28):((j+1)*28),(i*28):((i+1)*28)] = image\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_grid, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "visualize_decoder(myVAE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3\n",
    "\n",
    "* Make a *video* in any video or `gif` format to display the digits generated from the 2D latent space as frames and/or the progressive learning of the model (i.e. batch as the time axis).\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional ideas to explore\n",
    "* Implement a convolutional VAE. An example is available [here](https://github.com/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb).\n",
    "* Explore other VAEs following the [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE) repository to generate new faces from celebrities.\n",
    "* Study [A Tutorial on Variational Autoencoders with a Concise Keras Implementation](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/) by Louis Tiao and implement the same in pytorch.\n",
    "* Explore [PyroLab 1 - Bayesian Linear Regression for Generative Learning with Pyro](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%201%20-%20Bayesian%20Linear%20Regression%20for%20Generative%20Learning%20with%20Pyro.ipynb)\n",
    "* Explore [PyroLab 2 - Variational Autoencoder for Deep Generative Learning](https://github.com/haipinglu/SimplyDeep/blob/master/PyroLab%202%20-%20Variational%20Autoencoder%20for%20Deep%20Generative%20Learning.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
